# -*- coding: utf-8 -*-
# @author: Elie
"""
install as follows:
conda create --name drdamage_v1 --yes numpy scipy pandas matplotlib seaborn scikit-learn pysam xgboost; #gave xgboost=1.5.0, python=3.9.7
conda activate drdamage_v1;
pip install SigProfilerMatrixGenerator;
python -c "from SigProfilerMatrixGenerator import install as genInstall; genInstall.install('GRCh38', rsync=False, bash=True)"
"""

# Libraries
from SigProfilerMatrixGenerator.scripts import SigProfilerMatrixGeneratorFunc as matGen
import os
import glob
import shutil
from pathlib import Path
import argparse
import subprocess
from functools import partial, reduce
import datetime

import pandas as pd
pd.options.mode.chained_assignment = None
from matplotlib import pyplot as plt
import matplotlib as mpl
import xgboost
from darc_sign_pipeline.graphs import SNV_naive_signature, INDEL_naive_signature, CN_naive_signature

# for the cluster where X11 screen doesnt exist. posix for old cluster. 
if os.name == 'posix' and "DISPLAY" not in os.environ:
	mpl.use('Agg')
pd.options.mode.chained_assignment = None

def main():
	"""arguments that should all stay global and other constants"""
	parser = argparse.ArgumentParser()
	parser.add_argument("-m", "--maf", 
			    help="Path to input maf file", 
			    dest="maf_path", type=str, required=True)
	parser.add_argument("-s", "--seg", 
			    help="Path to input segments file generated by sequenza. Unless otherwise named, sequenza names this file with suffix _segments.txt", 
			    dest="seg_path", type=str, required=True)
	parser.add_argument("-od", "--output_dir", 
			    help="Output directory for all naive signature context vectors and associated graphs. Often this directory is named the same as the sample name eg /Path/to/output/samplename" , 
			    dest="out_path", type=str, required=True)
	parser.add_argument("-sn", "--sample_name", 
			    help="The name of the sample. This name will be used in resulting output tables and figures. It will also be used in the naming of all ouptut files" , 
			    dest="sn", type=str, required=True)
	parser.add_argument("-md", "--model_directory", 
			    help="directory containing the 3 dna repair models" , 
			    dest="model_path", type=str, required=False)
	args = parser.parse_args()

	mafpath = os.path.realpath(os.path.expanduser(args.maf_path))
	segpath = os.path.realpath(os.path.expanduser(args.seg_path))
	outpath = os.path.realpath(os.path.expanduser(args.out_path))
	rootdir = os.path.realpath(os.path.dirname(__file__))
	darcdir = os.path.join(rootdir, "darc_sign_pipeline")
	if args.model_path is not None:
		modelpath = os.path.realpath(os.path.expanduser(args.model_path))
	else: 
		modelpath = os.path.join(rootdir, "models") #so we can retrain and replace models.

	sample_name = str(args.sn)
	#make output directory incase isnt already made
	Path(outpath).mkdir(parents=True, exist_ok=True)
	os.chdir(outpath)

	"""make these variables global"""
	snv_categories = ["sample", 
					"A[C>A]A", "A[C>A]C", "A[C>A]G", "A[C>A]T", 
					"C[C>A]A", "C[C>A]C", "C[C>A]G", "C[C>A]T", 
					"G[C>A]A", "G[C>A]C", "G[C>A]G", "G[C>A]T", 
					"T[C>A]A", "T[C>A]C", "T[C>A]G", "T[C>A]T", 
					"A[C>G]A", "A[C>G]C", "A[C>G]G", "A[C>G]T", 
					"C[C>G]A", "C[C>G]C", "C[C>G]G", "C[C>G]T", 
					"G[C>G]A", "G[C>G]C", "G[C>G]G", "G[C>G]T", 
					"T[C>G]A", "T[C>G]C", "T[C>G]G", "T[C>G]T", 
					"A[C>T]A", "A[C>T]C", "A[C>T]G", "A[C>T]T", 
					"C[C>T]A", "C[C>T]C", "C[C>T]G", "C[C>T]T", 
					"G[C>T]A", "G[C>T]C", "G[C>T]G", "G[C>T]T", 
					"T[C>T]A", "T[C>T]C", "T[C>T]G", "T[C>T]T", 
					"A[T>A]A", "A[T>A]C", "A[T>A]G", "A[T>A]T", 
					"C[T>A]A", "C[T>A]C", "C[T>A]G", "C[T>A]T", 
					"G[T>A]A", "G[T>A]C", "G[T>A]G", "G[T>A]T", 
					"T[T>A]A", "T[T>A]C", "T[T>A]G", "T[T>A]T", 
					"A[T>C]A", "A[T>C]C", "A[T>C]G", "A[T>C]T", 
					"C[T>C]A", "C[T>C]C", "C[T>C]G", "C[T>C]T", 
					"G[T>C]A", "G[T>C]C", "G[T>C]G", "G[T>C]T", 
					"T[T>C]A", "T[T>C]C", "T[T>C]G", "T[T>C]T", 
					"A[T>G]A", "A[T>G]C", "A[T>G]G", "A[T>G]T", 
					"C[T>G]A", "C[T>G]C", "C[T>G]G", "C[T>G]T", 
					"G[T>G]A", "G[T>G]C", "G[T>G]G", "G[T>G]T", 
					"T[T>G]A", "T[T>G]C", "T[T>G]G", "T[T>G]T"]

	indel_categories = ["sample", 
					"1:Del:C:0", "1:Del:C:1", "1:Del:C:2", "1:Del:C:3", "1:Del:C:4", "1:Del:C:5", 
					"1:Del:T:0", "1:Del:T:1", "1:Del:T:2", "1:Del:T:3", "1:Del:T:4", "1:Del:T:5", 
					"1:Ins:C:0", "1:Ins:C:1", "1:Ins:C:2", "1:Ins:C:3", "1:Ins:C:4", "1:Ins:C:5", 
					"1:Ins:T:0", "1:Ins:T:1", "1:Ins:T:2", "1:Ins:T:3", "1:Ins:T:4", "1:Ins:T:5", 
					"2:Del:R:0", "2:Del:R:1", "2:Del:R:2", "2:Del:R:3", "2:Del:R:4", "2:Del:R:5", 
					"3:Del:R:0", "3:Del:R:1", "3:Del:R:2", "3:Del:R:3", "3:Del:R:4", "3:Del:R:5", 
					"4:Del:R:0", "4:Del:R:1", "4:Del:R:2", "4:Del:R:3", "4:Del:R:4", "4:Del:R:5", 
					"5:Del:R:0", "5:Del:R:1", "5:Del:R:2", "5:Del:R:3", "5:Del:R:4", "5:Del:R:5", 
					"2:Ins:R:0", "2:Ins:R:1", "2:Ins:R:2", "2:Ins:R:3", "2:Ins:R:4", "2:Ins:R:5", 
					"3:Ins:R:0", "3:Ins:R:1", "3:Ins:R:2", "3:Ins:R:3", "3:Ins:R:4", "3:Ins:R:5", 
					"4:Ins:R:0", "4:Ins:R:1", "4:Ins:R:2", "4:Ins:R:3", "4:Ins:R:4", "4:Ins:R:5", 
					"5:Ins:R:0", "5:Ins:R:1", "5:Ins:R:2", "5:Ins:R:3", "5:Ins:R:4", "5:Ins:R:5", 
					"2:Del:M:1", "3:Del:M:1", "3:Del:M:2", "4:Del:M:1", "4:Del:M:2", "4:Del:M:3", 
					"5:Del:M:1", "5:Del:M:2", "5:Del:M:3", "5:Del:M:4", "5:Del:M:5"]
					
	cnv_categories = ["sample", 
					"BCper10mb_0", "BCper10mb_1", "BCper10mb_2", "BCper10mb_3", 
					"CN_0", "CN_1", "CN_2", "CN_3", "CN_4", "CN_5", "CN_6", "CN_7", "CN_8", 
					"CNCP_0", "CNCP_1", "CNCP_2", "CNCP_3", "CNCP_4", "CNCP_5", "CNCP_6", "CNCP_7", 
					"BCperCA_0", "BCperCA_1", "BCperCA_2", "BCperCA_3", "BCperCA_4", "BCperCA_5", 
					"SegSize_0", "SegSize_1", "SegSize_2", "SegSize_3", "SegSize_4", "SegSize_5", 
					"SegSize_6", "SegSize_7", "SegSize_8", "SegSize_9", "SegSize_10", 
					"CopyFraction_0", "CopyFraction_1", "CopyFraction_2", "CopyFraction_3", "CopyFraction_4", 
					"CopyFraction_5", "CopyFraction_6"]

	"""functions that all get run in this little pipeline"""

	def maf2vcf():
		"""make outout directory incase isnt already made and process maf into vcf"""
		print(f"MAF2VCF of {sample_name} at {str(datetime.datetime.now())}")
		vcffile = f"{outpath}/{sample_name}.vcf"
		cmd = f"printf '##fileformat=VCFv4.1\n' > {vcffile}"
		subprocess.call(cmd, shell=True)
		cmd = f"printf '#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n' >> {vcffile}"
		subprocess.call(cmd, shell=True)
		cmd = f"tail -n +2 {mafpath} | awk \'BEGIN{{FS=OFS=\"\\t\"}} {{print $2,$3,\".\",$5,$6,\".\", \".\", \".\"}}\' >> {vcffile}"
		# print(cmd)
		subprocess.call(cmd, shell=True)

	def run_sigprofiler():
		"""run the counting tool on the new vcf"""
		print(f"SigProfiler generate SNV and Indel of {sample_name} at {str(datetime.datetime.now())}")
		matrices = matGen.SigProfilerMatrixGeneratorFunc(sample_name, "GRCh38", outpath , plot=True, exome=False, bed_file=None, 
								chrom_based=False, tsb_stat=False, seqInfo=False, cushion=100)
														
	def run_segprofilegenerator():
		"""run my counting script on the sequenza segments file"""
		print(f"Generate Segmentation feature matrix of {sample_name} at {str(datetime.datetime.now())}")
		make_copysig_script = os.path.join(darcdir, "CopySigMatrixGenerator.py")
		centrofile = os.path.join(darcdir, "cytoBandhg38.txt")
		segfile = segpath
		output_seg_path = os.path.realpath(f"{outpath}/segs")
		Path(output_seg_path).mkdir(parents=True, exist_ok=True)
		cmd = f"python {make_copysig_script} -s {segfile} -c {centrofile} -sn {sample_name} -o {output_seg_path};"
		subprocess.call(cmd, shell=True)

	def format_outputmatrices():
		"""turn sigprofiler outputs into logical tabular data so I can do operation on it"""
		print(f"Processing feature matrices of {sample_name} at {str(datetime.datetime.now())}")
		matrices_dir = os.path.realpath(f"{outpath}/matrices") #stick the raw counts in here so can be used by other tools
		Path(matrices_dir).mkdir(parents=True, exist_ok=True)

		cnv_to_process = os.path.realpath(f"{outpath}/segs/{sample_name}_copy_feature_matrix.tsv")
		cnv_final = os.path.realpath(f"{matrices_dir}/{sample_name}_cnv45feature_matrix.tsv")
		cnv = pd.read_csv(cnv_to_process, sep='\t', low_memory=False)
		cnv["sample"] = sample_name
		cnv = cnv[cnv_categories]
		cnv.to_csv(cnv_final, index=False, sep="\t")

		snv_to_process = os.path.realpath(f"{outpath}/output/SBS/{sample_name}.SBS96.all")
		snv_final = os.path.realpath(f"{matrices_dir}/{sample_name}_snv96feature_matrix.tsv")
		snv = pd.read_csv(snv_to_process, sep='\t', low_memory=False)
		values = snv[snv.columns[1]].to_list()
		muts = snv[snv.columns[0]].to_list()
		data = dict(zip(muts,values))
		snv_structured = pd.DataFrame(data, index=[0])
		snv_structured["sample"] = sample_name
		snv_structured_ordered = snv_structured[snv_categories]
		snv_structured_ordered.to_csv(snv_final, index=False, sep="\t")

		ndl_to_process = os.path.realpath(f"{outpath}/output/ID/{sample_name}.ID83.all")
		ndl_final = os.path.realpath(f"{matrices_dir}/{sample_name}_ndl83feature_matrix.tsv")
		ndl = pd.read_csv(ndl_to_process, sep='\t', low_memory=False)
		values = ndl[ndl.columns[1]].to_list()
		muts = ndl[ndl.columns[0]].to_list()
		data = dict(zip(muts,values))
		ndl_structured = pd.DataFrame(data, index=[0])
		ndl_structured["sample"] = sample_name
		ndl_structured_ordered = ndl_structured[indel_categories]
		ndl_structured_ordered.to_csv(ndl_final, index=False, sep="\t")

		my_reduce = partial(pd.merge, on='sample', how='outer')
		sample_features = reduce(my_reduce, [snv_structured_ordered, ndl_structured_ordered, cnv]) 
		return sample_features

	def predict_dnarepair(modelpath, feature_values, gene, sample_name):
		print(f"start predicting {gene} deficiency at {str(datetime.datetime.now())}")
		model_path = os.path.expanduser(modelpath)
		features_list = snv_categories[1:] + indel_categories[1:] + cnv_categories[1:]
		X_data = feature_values[features_list]
		#xgboost cant handle these characters
		X_data.columns = X_data.columns.str.replace("[", "mm").str.replace("]", "nn").str.replace(">", "rr")
		
		# instantiate classifier then load model onto it
		xgbmodel = xgboost.XGBClassifier()
		xgbmodel.load_model(model_path)
		# xgbmodel.save_model(f"{script_dir_path}/{gene}_xgb150_py397.model.txt")
		prediction_prob = xgbmodel.predict_proba(X_data, ntree_limit=1000000)
		df_probs = pd.DataFrame(data={"sample":sample_name, "prob_of_true": prediction_prob[:,1]})
		df_probs.index = [0] #it needs an index still to be put into df
		#change prob column to specific gene name
		prob_col = f"prob_of_{gene}"
		preds = df_probs.rename(columns={"prob_of_true": prob_col})
		preds = preds[["sample", prob_col]]
		print(f"finished predicting {gene} deficiency at {str(datetime.datetime.now())}")
		return preds

	def restructure_directories():
		# outpath=r"C:\Users\ElieRitch\Desktop\GU-18-203v4"
		matrices_dir = os.path.realpath(f"{outpath}/matrices")
		file_names = os.listdir(matrices_dir)
		for file_name in file_names:
			shutil.copy(os.path.join(matrices_dir, file_name), outpath)
		directories = next(os.walk(outpath))[1]
		for directory in directories:
			shutil.rmtree(os.path.join(outpath, directory))

	""" run the pipeline: create input files --> predict --> graph --> structure directories """

	maf2vcf()
	run_sigprofiler()
	run_segprofilegenerator()
	sample_features = format_outputmatrices()

	all_probabilites_list = [] #for merging the probability df's together

	gene="BRCA2d"
	gene_model = glob.glob(f"{modelpath}/{gene}*.txt")[0]
	predictions = predict_dnarepair(gene_model, sample_features, gene, sample_name)
	all_probabilites_list.append(predictions)

	gene="CDK12d"
	gene_model = glob.glob(f"{modelpath}/{gene}*.txt")[0]
	predictions = predict_dnarepair(gene_model, sample_features, gene, sample_name)
	all_probabilites_list.append(predictions)

	gene="MMRd"
	gene_model = glob.glob(f"{modelpath}/{gene}*.txt")[0]
	predictions = predict_dnarepair(gene_model, sample_features, gene, sample_name)
	all_probabilites_list.append(predictions)

	my_reduce = partial(pd.merge, on="sample", how='outer')
	all_prob_table = reduce(my_reduce, all_probabilites_list)

	matrices_dir = f"{outpath}/matrices"
	output_probs = f"{matrices_dir}/{sample_name}_probability_of_DRd.tsv"
	all_prob_table.to_csv(output_probs, index=False, sep="\t")

	SNV_naive_signature(sample_features)
	snv_fig = f"{matrices_dir}/{sample_name}_snv96feature_proportions"
	plt.savefig(f"{snv_fig}.png", dpi=500)
	plt.savefig(f"{snv_fig}.pdf", dpi=500)
	plt.close()

	INDEL_naive_signature(sample_features)
	ndl_fig = f"{matrices_dir}/{sample_name}_ndl83feature_proportions"
	plt.savefig(f"{ndl_fig}.png", dpi=500)
	plt.savefig(f"{ndl_fig}.pdf", dpi=500)
	plt.close()

	CN_naive_signature(sample_features)
	cnv_fig = f"{matrices_dir}/{sample_name}_cnv45feature_proportions"
	plt.savefig(f"{cnv_fig}.png", dpi=500)
	plt.savefig(f"{cnv_fig}.pdf", dpi=500)
	plt.close()

	restructure_directories()
if __name__ == "__main__":
	main()
